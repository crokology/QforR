---
title: "Code explanations"
author: "Christoph Schulze"
date: "27 September 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
It should be noted that everything we do here can also be done in the shiny app (web interface) of the *qmethod* package by Aiora Zabala [here](https://azabala.shinyapps.io/qmethod-gui/). All these guidelines and code are based on instructions laid out by Aiora Zabala [here](https://github.com/aiorazabala/qmethod/wiki/Cookbook).

My contribution here is simply to say one or two words what's happening in between the lines in a bit more detail.

***
### Contents
  * [0. R setup](#r-setup)
  * [1. Loading data](#loading-data-into-r)
  * [2. Basic analysis](#very-basic-analysis)
  * [3. Q analysis](#q-analysis)
  * [4. Extracting factors](#extracting-factors)
  * [5. Interpretation](#making-sense-of-the-extracted-factors)
  * [6. Saving results](#saving-results)
  * [References](#references)

***


### 0. R setup
First of all, make sure you installed the *qmethod* package by Aiora Zabala. 
```r
install.packages("qmethod")
```

In the beginning of your document, make sure to **call** the library.

```{r}
library(qmethod)
```
Next up, set the according working directory

```r
setwd("your_directory")
```

### 1. Loading data into R

So, an important note before importing the data (in that case here called *q_data.csv*), delete in excel first column and save as csv file. Optionally, you can save the data then in a separate file.

Here I will illustrate the different analytical steps with an expample data set, which I priorly saved in *q_data.csv*.

On this page I make some general remarks on what's happening in the code and also <span style="color: #0099cc;">introduce a few comments in blue which then refer to the output based on the sample data file.</span>
```{r}
qdata <- read.csv("q_data.csv", header=TRUE, sep=";",dec=".")

save(qdata,file = "qdata.RData")
```

### 2. Very basic analysis

Just to get a first idea of your data, having a quick look at the correlations. This is by far not a necessary step, but oftentimes nice to see who is correlated with whom. This shall provide some idea about which Q-sorts are most and least similar and therefore tell us something about how the factors are likely to develop.

```r
cor(qdata)
```

### 3. Q analysis

At this stage we will tell R: "Have a look at he data and check for 3 Factors". This is very crucial here. From now on we will use the features of the *qmethod* package.\
The *qmethod* function below goes throw our data. Based on statistical criteria mentioned further on, we will have to decide if we are ok with the number of factors, or if we have to decrease the number further down to 2.

```{r}
results <- qmethod(qdata, nfactors = 3, rotation = 'varimax', cor.method="pearson")
```

<span style="color: #0099cc;">In our example here, we extracted 3 factors via the *varimax* rotation and *pearson* correlation method.</span>

The rotation method can best be understood or lets say compared to a search mechanism under which the programme tries to identify the number of factors in the dataset. Now *rotation* might sound a little fancy, but it's actually quite accurate of what is happening here in the background. Lets remember briefly, our aim is to reduce the dimensions of our dataset. Within Q-studies we interview many people to find a reduced number of subjective viewpoints. This should be intuitive. However, we can tell the program based on which criteria it should reduce our dataset to end up which our desired subjective viewpoints. 

In the case of the varimax rotation, we tell the programme to rotate the dataset in a way that each factor explain as much variance as possible. In other words, the rotation goes into the direction in which the data is most dispersed. Consequently, we see that the first factor explains most, followed by the second, followed by the third... 

Varimax is thereby set as default. There are many other popular rotation modes, which are each based on different rotation criteria. Each mode has its own rationale. If you think that variance is not a good criterion for your data rotation, you might wanna have a look at different rotation methods, such as:

  * quatimax
  * promax
  * oblimin
  * simplimax
  
or do the roation by hand.

### 4. Extracting factors

Watts and Stenner (2012) provide a good overview of Q-methodology and the respective criteria that determine the number of extracted factors.

First we check which participants load onto which factor.

```{r}
results$flag

loa.and.flags(results)
```

Among one of these criteria mentioned by Watts and Stenner (2012) is the number of Q-sorts belonging to a factor. The commands above return the Q-sorts which significantly load onto a certain factor. An important indicator for that are the so called **factor loadings**. Factor loadings are the scores indicating how much each Q sort loads on each factor. Moreover, a Q-sort, which signifiantly loads onto a factor is called *flagged*.

<span style="color: #0099cc;">Again, in our example the stars indicate which sort is flagged for which factor and the numbers are the respective factor loadings. As we can see, for the first factor we have six flagged Q-sorts, for the second factor five flagged Q-sorts and for the third factor we have three flagged Q-sorts. We are good to go.</span>

Each Q-sort can only load onto one factor. However, it might occur that one Q-sort may load relatively high on multiple factors. In that case, we cannot clearly attribute the person's Q-sort to one sinle factor, leading to a potential exclusion of that Q-sort. As stated by Zabala (2014), there are two relevant criteria for flagging a Q-sort:

 * 1) qsorts which factor loading is higher than the threshold for pval >0.95, and 
 * 2) qsorts which square loading is higher than the sum of square loadings of the same q-sort in all other factors
 
More precisely, regarding 1) we calculate the significance level by $1.96 * \frac{1}{\sqrt{n}}$, where $n$ = number of statements. For what concerns the second test, the idea is to test if a Q-sort loads onto a single factor by a large enough margin that it can be considered to be a factor exemplar.

```{r}
results$f_char$characteristics
```
Other criteria can be found in the table above. Based on Watts and Stenner (2012), the following criteria play a role in determining the number of factors extracted from the analysis:

  * minimum number of Q-sorts $\geq$ 2 *(what we did above)*
  * Eigenvalue $\geq$ 1 (Kaiser Guttman criterion)
  * share of accumulated explained variance of extracted factors $\geq$ 35%
  * Humphreys rule
  * Screeplot

<span style="color: #0099cc;">Lets go through them one by one. Eigenvalues are all bigger 1 and accumulated variance amounts to 63%. In short, so far all criteria are satisfied.</span>

The **Kaiser Guttman** criterion ensures each extracted factor accounts for at least as much study variance than a single Q sort. If this was not the case, the factor in question captures less information than the data provided by a single participant.

Humphreys rule basically states that the cross product of the two highest factor loadings must exceed $\frac{-2}{\sqrt{n}}$

```{r}
humphrey<-2/sqrt(dim(qdata))
humphrey
```

<span style="color: #0099cc;">The first number of this command gives us the threshold of the *Humphrey rule*. If we calculate this for our third factor, we get 0,64. Since this is larger than the calculated threshold, we can say that also that criteria is satisfied.</span>

The last criteria is a graphical description of variance explained by additional factors. Here, a kink in the line should indicate a cut-off point of additional factors.

```{r}
screeplot(prcomp(qdata), main = "Screeplot of unrotated factors", type = "l")
```

<span style="color: #0099cc;">Okay, this criterion leaves some room for interpretation, but what are we looking for? What we are trying to do is to extract a number of factors that is smaller than the number of participants in our study. At the same time, we try to explain as much variance as possible. A kink in the screeplot basically tells us: that factor is not explaining much viariance. What is a kink and what not is subject to individual interpretation though.</span>

### 5. Making sense of the extracted factors
At this stage we know how many factors we extract from the data. Now, the task is to build a narrative and fill them with meaning.\
The following line of code will provide us with a whole bunch of information, such as:

  * size of data set
  * number of extracted factors
  * factor loadings
  * flagged Q-sorts
  * statement z-scores
  * factor characteristics
  * factor correlations
  * distinguishing and consensus statements


```r
summary(results)
results
```
So far, we looked at most of these bits of information. Now, for interpretation of the respective factors we will turn to the distinguishing and consensus statements. In short, consensus statements are elements of general agreement, whereas distinguishing statements indicate potential areas of contention.

Now, how do we know if a statement is distinguished or represents consensus? Simple, by comparing **z-scores**. Z-scores tell us, how a statement has been evaluated on average by study participants for each respective factor. Consequently, these scores have the same dimensions as the extremes of the applied Q-grid.

Whether or not a statement is significantly distinguished by a factor depends on the standard error of the difference of the respective z-score of that statement. This measure indicates if two factors evaluated a certain statement differently, by looking at the dispersion of the z-scores for each factor.
```{r}
plot(results)
```

Here we see a graphical representation of the z-scores for each factor. At the bottom we see consensus statements and at the top statements with the highest dispersion of z-scores. If an icon is filled with a color, we know that the factor is significantly distinguished for that respective statement.

<span style="color: #0099cc;">In the example above, the grid ranged from -4 to 4. Thus, the z-scores must also be located within that range. Each factor here has its own color. As said before, at the bottom we find consensus statements and at the top statements which are highly distinguished. Note that the more up we go, the likelier it is that a colored object is actually filled with color and not just outlined. This is because the more dispersed the z-scores, the likelier it is that the standard error of the difference exceeds the critical point to appear significant.</span>

Another useful feature to interprete the different factors is to build **idealised Q-sorts**. This idealised Q-sort represents a so to say average of a Q-sort of a factor.

```{r}
scores <- cbind(round(results$zsc, digits=2), results$zsc_n) 
nfactors <- ncol(results$zsc) 
col.order <- as.vector(rbind(1:nfactors, (1:nfactors)+nfactors)) 
scores <- scores[col.order] 
scores
```

Reordering might help to see which statments have been most polarising for each factor

```{r}
scores[order(scores$zsc_f1, decreasing = T), ]
scores[order(scores$zsc_f2, decreasing = T), ] 
scores[order(scores$zsc_f3, decreasing = T), ]
```

Apart from the graphical representation, we can also directly assess the differences of the z-scores for each particular statement. The next line of code tells you whether a statement:

  * is consensus
  * distinguishes all
  * or distinguishes a particular factor only
  
In addition, the differences will be shown.
```{r}
results$qdc
```

<span style="color: #0099cc;">Note that there are a few instances which are blank. This represents some sort of "middleground". In other words, there are differences in z-scores which lead us to assume that there is no consensus. However, these differences are not large enough to speak of "distinguished statements".</span>

To make things a little easier, we can order the statements based on their categorisation.

Clearly, these categorisations are key for building the respective narratives. Personally, I have a separate excel spreadsheet in which I mark these features. In the end, it is the idealised Q-sorts in combination with the features below that really help to draw a picture of the different narratives.

```{r}
results$qdc[which(results$qdc$dist.and.cons == "Consensus"), ]

results$qdc[which(results$qdc$dist.and.cons == "Distinguishes all"), ]

results$qdc[which(results$qdc$dist.and.cons == "Distinguishes f1 only"), ]

results$qdc[which(results$qdc$dist.and.cons == "Distinguishes f2 only"), ]

results$qdc[which(results$qdc$dist.and.cons == "Distinguishes f3 only"), ]
```

Another way of highlighting factor differences is to compute the pairwise **z-score differences** between two factors. This operation is already being done and the differences are storred in the background, we just need to call it with the right command. Here we are interested in the differences between Factor 1 and Factor 2 and order the z-score differences respectively.

```{r}
results$qdc[order(results$qdc$f1_f2, decreasing = T), ]
```

Apart from the z-score differences, we also see whether or not these differences are significant or not. By changing *f1_f2* to *f1_f3* we order the z-score differences of Factor 1 and 3 respectively.

### 6. Saving results
```r
save(results, file = "practiseresults.Rdata")

write.csv(results$zsc, file = "zscores.csv") 

write.csv(results$zsc_n, file = "factorscores.csv") 

write.csv(results$loa, file = "loadings.csv")
```

These files will incorporate all things we just did. This is pretty useful in case you want to look at your results, but do not want to run R again to do so.

```r
export.qm(results, file = "myreport.txt", style = "R")

export.qm(results, file = "myreport-pqm.txt", style = "PQMethod")
```
### References

Watts, S., & Stenner, P. (2012). Doing Q methodological research: Theory, method & interpretation. Sage.

Zabala, A. (2014). qmethod: A package to explore human perspectives using Q methodology.
